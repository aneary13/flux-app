# FLUX Database (Supabase)

This directory contains the database configuration, schema migrations, and infrastructure settings for the FLUX application. It relies on the [Supabase CLI](https://supabase.com/docs/guides/cli) to manage local development and remote deployments.

## Architecture: The Hybrid Relational-JSON Schema

The FLUX database uses PostgreSQL. To accommodate the highly variable data generated by different biological training protocols (e.g., strength vs. conditioning), we use a hybrid schema approach to avoid sparse, wide tables.

### Core Tables

1. **`user_configs`**: Stores the "Brain" of the app.
   * Contains YAML-derived logic (`logic`, `selections`, `sessions`, `conditioning`).
   * Contains the dynamic user `state` (pattern debts, conditioning progression levels).
2. **`exercises`**: The catalog of validated movements and their metadata (tracking units, load types).
3. **`workout_sessions`**: Tracks active and completed training sessions, mapping to an archetype (Performance/Recovery) and an anchor pattern.
4. **`workout_sets`**: Logs individual sets. 
   * **Hybrid Column Strategy:** Standard metrics (`weight`, `reps`, `seconds`) are stored in root columns. Highly specialized or protocol-specific metrics (e.g., `avg_watts`, `peak_hr`, `protocol`, `level`) are stored in a `metadata` (JSONB) column.

## Local Development Workflow

We use Docker and the Supabase CLI to run a full replica of the production database on your local machine.

### 1. Start the Local Stack

Make sure Docker Desktop is running, then execute:

```bash
supabase start
```

*This spins up PostgreSQL, the API Gateway, and the Supabase Studio UI (typically available at `http://127.0.0.1:54323`).*

### 2. Stop the Local Stack

To save RAM when you are done working:

```bash
supabase stop
```

## Managing Schema Migrations

Never edit the remote database schema directly through the UI. All schema changes must be tracked in version control via migrations.

### The Standard Workflow

1. **Create a new migration file:**

```bash
supabase migration new descriptive_name_of_change
```

2. **Write your SQL:** Open the generated file in `supabase/migrations/` and add your `CREATE TABLE` or `ALTER TABLE` statements.

3. **Test locally:**

```bash
supabase migration up
```

4. **Deploy to production:** (Ensure you are linked to your remote project via `supabase link`)

```bash
supabase db push
```

### Troubleshooting: "Empty" or Failed Migrations

If you accidentally push an empty migration file and Supabase thinks it is "applied," you can repair the migration history:

```bash
supabase migration repair --status reverted <timestamp_of_migration>
```

Once reverted, fix the SQL file and run `supabase db push` again.

## Resetting & Seeding the Database

During testing, you may need to wipe the slate clean and start from "Day 1."

### 1. Wiping the Tables (Clean Slate)

To completely erase user history and configuration data without destroying the tables themselves, run this in the Supabase SQL Editor:

```sql
TRUNCATE TABLE workout_sets CASCADE;
TRUNCATE TABLE workout_sessions CASCADE;
TRUNCATE TABLE user_configs CASCADE;
TRUNCATE TABLE exercises CASCADE;
```

### 2. Repopulating the "Brain"

Once truncated, the database needs the logic and exercises re-inserted. Navigate to the `backend/` directory and run the seed script:

```bash
uv run scripts/seed_db.py
```

*(Warning: Ensure your `backend/.env` file is pointing to the correct environment—Local vs. Remote—before running this script!)*

## Security & Access Control

* **Row Level Security (RLS):** RLS must be enabled on all tables in production.
* **API Access:** The FastAPI backend interacts with Supabase using the standard `anon` key. Therefore, RLS policies must explicitly allow `SELECT`, `INSERT`, and `UPDATE` operations for the anonymous role where appropriate, or API calls will silently fail and return empty lists `[]`.
